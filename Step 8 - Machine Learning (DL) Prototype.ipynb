{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f12dfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import json, re, nltk, string\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4237c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d52d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_frequency_word2vec = 5\n",
    "embed_size_word2vec = 200\n",
    "context_window_word2vec = 5\n",
    "\n",
    "numCV = 10\n",
    "max_sentence_len = 50\n",
    "min_sentence_length = 15\n",
    "rankK = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb565aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('classifier_data_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "add44ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = df.groupby('owner')['owner'].filter(lambda x: len(x) >= 500)\n",
    "f = df[df['owner'].isin(filtered)]\n",
    "df = f\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()\n",
    "len(f['owner'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6b5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.description\n",
    "y = df.owner\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01570a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = []\n",
    "train_owner = []\n",
    "test_data = []\n",
    "test_owner = []\n",
    "\n",
    "all_data_unfiltered = []\n",
    "\n",
    "def purge_string(text):\n",
    "    current_desc = text.replace('\\r', ' ')    \n",
    "    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "    start_loc = current_desc.find(\"Stack trace:\")\n",
    "    current_desc = current_desc[:start_loc]    \n",
    "    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n",
    "    current_desc = current_desc.lower()\n",
    "    current_desc_tokens = nltk.word_tokenize(current_desc)\n",
    "    current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]\n",
    "    current_data = current_desc_filter\n",
    "    return current_data\n",
    "\n",
    "for item in X_train:\n",
    "    current_data = purge_string(item)\n",
    "    all_data_unfiltered.append(current_data)     \n",
    "    train_data.append(filter(None, current_data)) \n",
    "\n",
    "for item in y_train:\n",
    "    train_owner.append(item)\n",
    "    \n",
    "for item in X_test:\n",
    "    current_data = purge_string(item)\n",
    "    test_data.append(filter(None, current_data)) \n",
    "\n",
    "for item in y_test:\n",
    "    test_owner.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f773719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = word2vec.Word2Vec(min_count=min_word_frequency_word2vec, vector_size=embed_size_word2vec, window=context_window_word2vec)\n",
    "model.init_sims(replace=True)\n",
    "model.build_vocab(all_data_unfiltered, progress_per=100000)\n",
    "vocabulary = model.wv.key_to_index\n",
    "vocab_size = len(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_train_data = []    \n",
    "updated_train_data_length = []    \n",
    "updated_train_owner = []\n",
    "final_test_data = []\n",
    "final_test_owner = []\n",
    "for j, item in enumerate(train_data):\n",
    "    current_train_filter = [word for word in item if word in vocabulary]\n",
    "    if len(current_train_filter)>=min_sentence_length:  \n",
    "      updated_train_data.append(current_train_filter)\n",
    "      updated_train_owner.append(train_owner[j])  \n",
    "      \n",
    "for j, item in enumerate(test_data):\n",
    "    current_test_filter = [word for word in item if word in vocabulary]  \n",
    "    if len(current_test_filter)>=min_sentence_length:\n",
    "        final_test_data.append(current_test_filter)\n",
    "        final_test_owner.append(test_owner[j]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7631420",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_label = list(set(updated_train_owner))\n",
    "classes = np.array(unique_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.empty(shape=[len(updated_train_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n",
    "Y_train = np.empty(shape=[len(updated_train_owner),1], dtype='int32')\n",
    "\n",
    "for j, curr_row in enumerate(updated_train_data):\n",
    "    sequence_cnt = 0         \n",
    "    for item in curr_row:\n",
    "        if item in vocabulary:\n",
    "            X_train[j, sequence_cnt, :] = model.wv[item] \n",
    "            sequence_cnt = sequence_cnt + 1                \n",
    "            if sequence_cnt == max_sentence_len-1:\n",
    "                    break                \n",
    "    for k in range(sequence_cnt, max_sentence_len):\n",
    "        X_train[j, k, :] = np.zeros((1,embed_size_word2vec))        \n",
    "    Y_train[j,0] = unique_train_label.index(updated_train_owner[j])\n",
    "\n",
    "X_test = np.empty(shape=[len(final_test_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n",
    "Y_test = np.empty(shape=[len(final_test_owner),1], dtype='int32')\n",
    "\n",
    "for j, curr_row in enumerate(final_test_data):\n",
    "    sequence_cnt = 0          \n",
    "    for item in curr_row:\n",
    "        if item in vocabulary:\n",
    "            X_test[j, sequence_cnt, :] = model.wv[item] \n",
    "            sequence_cnt = sequence_cnt + 1                \n",
    "            if sequence_cnt == max_sentence_len-1:\n",
    "                break                \n",
    "    for k in range(sequence_cnt, max_sentence_len):\n",
    "        X_test[j, k, :] = np.zeros((1,embed_size_word2vec))        \n",
    "    Y_test[j,0] = unique_train_label.index(final_test_owner[j])\n",
    "\n",
    "y_train = np_utils.to_categorical(Y_train, len(unique_train_label))\n",
    "y_test = np_utils.to_categorical(Y_test, len(unique_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for item in updated_train_data:\n",
    "    train_data.append(' '.join(item))\n",
    "\n",
    "test_data = []\n",
    "for item in final_test_data:\n",
    "    test_data.append(' '.join(item))\n",
    "\n",
    "vocab_data = []\n",
    "for item in vocabulary:\n",
    "    vocab_data.append(item)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "count_vect = CountVectorizer(min_df=1, vocabulary= vocab_data,dtype=np.int32)\n",
    "\n",
    "train_counts = count_vect.fit_transform(train_data)       \n",
    "train_feats = tfidf_transformer.fit_transform(train_counts)\n",
    "print (train_feats.shape)\n",
    "\n",
    "test_counts = count_vect.transform(test_data)\n",
    "test_feats = tfidf_transformer.transform(test_counts)\n",
    "print (test_feats.shape)\n",
    "print (\"=======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5716db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = cosine_similarity(test_feats, train_feats)\n",
    "classes = np.array(updated_train_owner)\n",
    "classifierModel = []\n",
    "\n",
    "accuracy = []\n",
    "sortedIndices = []\n",
    "pred_classes = []\n",
    "for ll in predict:\n",
    "    sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n",
    "for k in range(1, rankK+1):\n",
    "    id = 0\n",
    "    trueNum = 0\n",
    "    for sortedInd in sortedIndices:  \n",
    "#         print(sortedInd)\n",
    "#         if y_test[id] in classes[sortedInd[:k]]:\n",
    "        if final_test_owner[id] in classes[sortedInd[:k]]:\n",
    "            trueNum += 1\n",
    "            pred_classes.append(classes[sortedInd[:k]])\n",
    "        id += 1\n",
    "    accuracy.append((float(trueNum) / len(predict)) * 100)\n",
    "print (accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
